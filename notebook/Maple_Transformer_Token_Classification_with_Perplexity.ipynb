{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Maple - Transformer Token Classification with Perplexity",
      "provenance": [],
      "collapsed_sections": [
        "X2j1gjgj7r4D",
        "09TpS-xDwboX",
        "l5QODxUIy8qh",
        "g9U4aLJ9c-xu",
        "YTarAtZ6378d"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.7 64-bit ('tf': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "00bcf6c82b59e9741daa5a3d78becd13b5b298f7d43a2b1ea4edd03905860d8c"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb#scrollTo=GWiVUF0jIrIv\n",
        "\n",
        "https://huggingface.co/transformers/custom_datasets.html#token-classification-with-w-nut-emerging-entities\n",
        "\n",
        "https://huggingface.co/blog/ray-tune\n"
      ],
      "metadata": {
        "id": "jbyGVO0YIg6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing and importing libraries"
      ],
      "metadata": {
        "id": "X2j1gjgj7r4D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# !sudo apt install git-lfs\r\n",
        "# !git config --global user.email aditeya.baral@gmail.com\r\n",
        "# !git config --global user.name Aditeya\r\n",
        "HF_HUB_API_TOKEN = \"api_vOKpxbhcEnOqvVNYfPxOSzFoMsFNxOqttx\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.3.4-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT6pxvUmoLiN",
        "outputId": "0531edbb-db56-486c-ff04-1f714bd01ae8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install transformers datasets seqeval ray"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.12.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: ray in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.4.post0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: redis>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ray) (3.5.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.2.0)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.40.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.6.3)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNyQKKbz8KWj",
        "outputId": "1379ed74-e71e-4b6f-c561-7c9c5dc919e6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import nltk\r\n",
        "nltk.download(\"punkt\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNGsoLx3zYOM",
        "outputId": "03d1fb75-33e8-486d-f701-80d6c68b3c6f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import json\r\n",
        "import math\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from transformers import AutoTokenizer\r\n",
        "from datasets import load_dataset, Dataset, load_metric, DatasetDict\r\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\r\n",
        "from transformers import DataCollatorForTokenClassification\r\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, RobertaTokenizerFast"
      ],
      "outputs": [],
      "metadata": {
        "id": "mwHOf0lcudI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset"
      ],
      "metadata": {
        "id": "09TpS-xDwboX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = pd.read_json('/content/drive/MyDrive/Maple/ppl-grammar-dataset.json')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54629, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>poem</th>\n",
              "      <th>haiku</th>\n",
              "      <th>indices</th>\n",
              "      <th>ppl-gpt2</th>\n",
              "      <th>grammar-check</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Did the CIA tell the FBI that it knows the wor...</td>\n",
              "      <td>cia fbi the biggest weapon</td>\n",
              "      <td>[2, 5, 9, 24, 25]</td>\n",
              "      <td>2447.346480</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Did the CIA tell the FBI that it knows the wor...</td>\n",
              "      <td>cia fbi the biggest weapon</td>\n",
              "      <td>[2, 5, 9, 24, 25]</td>\n",
              "      <td>2447.346480</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dark clouds gathered overhead,\\nExpelling bull...</td>\n",
              "      <td>clouds overhead bullets of the valley</td>\n",
              "      <td>[1, 3, 5, 6, 10, 11]</td>\n",
              "      <td>3639.095887</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A vigilante lacking of heroic qualities that\\n...</td>\n",
              "      <td>lacking qualities that damn criminals</td>\n",
              "      <td>[2, 5, 6, 11, 12]</td>\n",
              "      <td>8305.684147</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(A Diamante Poem)\\nBrain\\nHeavenly, hellish\\nF...</td>\n",
              "      <td>diamante poem the sybaritic pathetic</td>\n",
              "      <td>[1, 2, 10, 18, 19]</td>\n",
              "      <td>1982.106818</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                poem  ... grammar-check\n",
              "0  Did the CIA tell the FBI that it knows the wor...  ...         False\n",
              "1  Did the CIA tell the FBI that it knows the wor...  ...         False\n",
              "2  Dark clouds gathered overhead,\\nExpelling bull...  ...         False\n",
              "3  A vigilante lacking of heroic qualities that\\n...  ...         False\n",
              "4  (A Diamante Poem)\\nBrain\\nHeavenly, hellish\\nF...  ...         False\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "n0QOAl4jwOOm",
        "outputId": "aeaf5832-53bd-4003-a84b-026b6aabe0b6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df[\"ppl-gpt2\"].describe()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     54629.000000\n",
              "mean       6511.533484\n",
              "std       13967.007052\n",
              "min           4.944884\n",
              "25%        1245.401788\n",
              "50%        2755.530497\n",
              "75%        6398.639543\n",
              "max      560770.619276\n",
              "Name: ppl-gpt2, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mCmC9UYwlC0",
        "outputId": "f4f5e6a6-327a-48b4-acd4-e12f2d59e3aa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df[\"grammar-check\"].describe()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     54629\n",
              "unique        2\n",
              "top       False\n",
              "freq      54557\n",
              "Name: grammar-check, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIfwU5zQxvHP",
        "outputId": "e53026dc-fbc8-4f53-9dab-fe63dbe45fea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some hindi sentences here - clean them in the preprocessing notebook"
      ],
      "metadata": {
        "id": "hALdVGvrx_ye"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "filtered_df = df[df[\"ppl-gpt2\"] <= 1500]\n",
        "filtered_df.reset_index(drop=True, inplace=True)\n",
        "print(filtered_df.shape)\n",
        "filtered_df.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16576, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>poem</th>\n",
              "      <th>haiku</th>\n",
              "      <th>indices</th>\n",
              "      <th>ppl-gpt2</th>\n",
              "      <th>grammar-check</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You sit left to me,\\nYou don't wanna see,\\nWha...</td>\n",
              "      <td>name extentions problem was about a tightrope</td>\n",
              "      <td>[38, 53, 56, 60, 75, 89, 90]</td>\n",
              "      <td>850.798879</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Life,\\nFrom ABC to XYZ;\\nLearning all the time...</td>\n",
              "      <td>abc xyz all perfect laws of all rules</td>\n",
              "      <td>[2, 4, 6, 11, 14, 15, 18, 20]</td>\n",
              "      <td>1279.585696</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Life,\\nFrom ABC to XYZ;\\nLearning all the time...</td>\n",
              "      <td>perfect laws all rules like the beginning</td>\n",
              "      <td>[11, 14, 18, 20, 24, 25, 26]</td>\n",
              "      <td>1097.296416</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Alfons Schuhbeck top chef.\\nBavaria, Germany i...</td>\n",
              "      <td>chef chefs a wonderful cookbook</td>\n",
              "      <td>[3, 11, 14, 15, 16]</td>\n",
              "      <td>974.032754</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Actress Angela Oberer passing a moment, Angela...</td>\n",
              "      <td>moment eyes a long time</td>\n",
              "      <td>[5, 27, 34, 35, 36]</td>\n",
              "      <td>445.035082</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                poem  ... grammar-check\n",
              "0  You sit left to me,\\nYou don't wanna see,\\nWha...  ...         False\n",
              "1  Life,\\nFrom ABC to XYZ;\\nLearning all the time...  ...         False\n",
              "2  Life,\\nFrom ABC to XYZ;\\nLearning all the time...  ...         False\n",
              "3  Alfons Schuhbeck top chef.\\nBavaria, Germany i...  ...         False\n",
              "4  Actress Angela Oberer passing a moment, Angela...  ...         False\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "Z884Fm1mwxEm",
        "outputId": "b4fc8644-c6d6-49d2-a704-1c3f2fd5d6b9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Dataset"
      ],
      "metadata": {
        "id": "l5QODxUIy8qh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "filtered_df[\"tokens\"] = filtered_df[\"poem\"].apply(lambda x: word_tokenize(x))\n",
        "ner_tags = list()\n",
        "for i in range(filtered_df.shape[0]):\n",
        "  indices = filtered_df[\"indices\"][i]\n",
        "  length = len(filtered_df[\"tokens\"][i])\n",
        "  ner_tag = ['O' for _ in range(length)]\n",
        "  for idx in indices:\n",
        "    ner_tag[idx] = 'W'\n",
        "  ner_tags.append(ner_tag)\n",
        "filtered_df[\"ner_tags\"] = ner_tags"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1SdvncGxe4U",
        "outputId": "824e420c-02b1-4971-879a-9ef57dfd18cc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "filtered_df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>poem</th>\n",
              "      <th>haiku</th>\n",
              "      <th>indices</th>\n",
              "      <th>ppl-gpt2</th>\n",
              "      <th>grammar-check</th>\n",
              "      <th>tokens</th>\n",
              "      <th>ner_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You sit left to me,\\nYou don't wanna see,\\nWha...</td>\n",
              "      <td>name extentions problem was about a tightrope</td>\n",
              "      <td>[38, 53, 56, 60, 75, 89, 90]</td>\n",
              "      <td>850.798879</td>\n",
              "      <td>False</td>\n",
              "      <td>[You, sit, left, to, me, ,, You, do, n't, wan,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Life,\\nFrom ABC to XYZ;\\nLearning all the time...</td>\n",
              "      <td>abc xyz all perfect laws of all rules</td>\n",
              "      <td>[2, 4, 6, 11, 14, 15, 18, 20]</td>\n",
              "      <td>1279.585696</td>\n",
              "      <td>False</td>\n",
              "      <td>[Life, ,, From, ABC, to, XYZ, ;, Learning, all...</td>\n",
              "      <td>[O, O, W, O, W, O, W, O, O, O, O, W, O, O, W, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Life,\\nFrom ABC to XYZ;\\nLearning all the time...</td>\n",
              "      <td>perfect laws all rules like the beginning</td>\n",
              "      <td>[11, 14, 18, 20, 24, 25, 26]</td>\n",
              "      <td>1097.296416</td>\n",
              "      <td>False</td>\n",
              "      <td>[Life, ,, From, ABC, to, XYZ, ;, Learning, all...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, W, O, O, W, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Alfons Schuhbeck top chef.\\nBavaria, Germany i...</td>\n",
              "      <td>chef chefs a wonderful cookbook</td>\n",
              "      <td>[3, 11, 14, 15, 16]</td>\n",
              "      <td>974.032754</td>\n",
              "      <td>False</td>\n",
              "      <td>[Alfons, Schuhbeck, top, chef, ., Bavaria, ,, ...</td>\n",
              "      <td>[O, O, O, W, O, O, O, O, O, O, O, W, O, O, W, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Actress Angela Oberer passing a moment, Angela...</td>\n",
              "      <td>moment eyes a long time</td>\n",
              "      <td>[5, 27, 34, 35, 36]</td>\n",
              "      <td>445.035082</td>\n",
              "      <td>False</td>\n",
              "      <td>[Actress, Angela, Oberer, passing, a, moment, ...</td>\n",
              "      <td>[O, O, O, O, O, W, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                poem  ...                                           ner_tags\n",
              "0  You sit left to me,\\nYou don't wanna see,\\nWha...  ...  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
              "1  Life,\\nFrom ABC to XYZ;\\nLearning all the time...  ...  [O, O, W, O, W, O, W, O, O, O, O, W, O, O, W, ...\n",
              "2  Life,\\nFrom ABC to XYZ;\\nLearning all the time...  ...  [O, O, O, O, O, O, O, O, O, O, O, W, O, O, W, ...\n",
              "3  Alfons Schuhbeck top chef.\\nBavaria, Germany i...  ...  [O, O, O, W, O, O, O, O, O, O, O, W, O, O, W, ...\n",
              "4  Actress Angela Oberer passing a moment, Angela...  ...  [O, O, O, O, O, W, O, O, O, O, O, O, O, O, O, ...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhWWw1LHzHNd",
        "outputId": "9474de8c-c467-48d3-cefe-fda726b87762"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_df, test_df = train_test_split(filtered_df, test_size=0.05)\n",
        "print(train_df.shape, test_df.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15747, 7) (829, 7)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D7ZU0dS1wL0",
        "outputId": "95e88b0d-ab04-4220-ceb3-25e21011ac62"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tokens = filtered_df[\"tokens\"]\n",
        "tags = filtered_df[\"ner_tags\"]"
      ],
      "outputs": [],
      "metadata": {
        "id": "WK4k1k4YFsvy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "unique_tags = [\"O\", \"W\"]\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "outputs": [],
      "metadata": {
        "id": "0sNQcvxsF0uz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ],
      "outputs": [],
      "metadata": {
        "id": "YxEy-Bd4-1ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialising GPT-2 for Perplexity Loss"
      ],
      "metadata": {
        "id": "g9U4aLJ9c-xu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "gpt2_model_name = \"gpt2\"\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
        "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(gpt2_model_name)\n",
        "\n",
        "def perplexityGPT2(sentences):\n",
        "  ppl = list()\n",
        "  total_length = len(sentences)\n",
        "  for index, sent in enumerate(sentences):\n",
        "    tokenize_input = gpt2_tokenizer.encode(sent)\n",
        "    tensor_input = torch.tensor([tokenize_input])\n",
        "    loss = gpt2_model(tensor_input, labels=tensor_input)[0]\n",
        "    ppl.append(math.exp(loss))\n",
        "  return ppl"
      ],
      "outputs": [],
      "metadata": {
        "id": "d3051LjVPrGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Model Parameters"
      ],
      "metadata": {
        "id": "gNKBMSX71sex"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model_name = \"bert-large-cased\""
      ],
      "outputs": [],
      "metadata": {
        "id": "1FxXFCeHBR88"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if \"roberta\" in model_name:\n",
        "  tokenizer = RobertaTokenizerFast.from_pretrained(model_name, add_prefix_space=True)\n",
        "else:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "outputs": [],
      "metadata": {
        "id": "LBssIbi2zQ7N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_texts = list(train_df[\"tokens\"].values)\n",
        "val_texts = list(test_df[\"tokens\"].values)\n",
        "\n",
        "train_tags = list(train_df[\"ner_tags\"].values)\n",
        "val_tags = list(test_df[\"ner_tags\"].values)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9w1OAErnGDXk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "p8O_9mLcKJeH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def encode_tags(tags, encodings):\n",
        "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
        "    encoded_labels = []\n",
        "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
        "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
        "        arr_offset = np.array(doc_offset)\n",
        "\n",
        "        try:\n",
        "          doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
        "          encoded_labels.append(doc_enc_labels.tolist())\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "    return encoded_labels\n",
        "\n",
        "train_labels = encode_tags(train_tags, train_encodings)\n",
        "val_labels = encode_tags(val_tags, val_encodings)"
      ],
      "outputs": [],
      "metadata": {
        "id": "1d08s4CBG1vt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class MapleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "offset_mapping_train = train_encodings.pop(\"offset_mapping\")\n",
        "offset_mapping_val = val_encodings.pop(\"offset_mapping\")\n",
        "train_dataset = MapleDataset(train_encodings, train_labels)\n",
        "val_dataset = MapleDataset(val_encodings, val_labels)"
      ],
      "outputs": [],
      "metadata": {
        "id": "lbVuzJyqKzGP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "'''def model_init():\n",
        "  return AutoModelForTokenClassification.from_pretrained(model_name, num_labels=2, return_dict=True)'''\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeUyLMKs9Oyu",
        "outputId": "993b01c9-e2d0-4ef0-cf66-cf53a39034f7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class PerplexityTrainer(Trainer):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        \n",
        "  def compute_loss(self, model, inputs, return_outputs=False):\n",
        "    batch_size = len(inputs[\"input_ids\"])\n",
        "    poems = list()\n",
        "    for i in range(batch_size):\n",
        "      text = tokenizer.decode(inputs[\"input_ids\"][i])\n",
        "      tokenized_text = tokenizer.tokenize(text)\n",
        "      labels = inputs[\"labels\"][i].tolist()\n",
        "      words = list()\n",
        "      for idx, value in enumerate(labels):\n",
        "        if value == 1:\n",
        "          words.append(tokenized_text[idx])\n",
        "      poem = ' '.join(words)\n",
        "      poems.append(poem)\n",
        "    perplexity_scores = perplexityGPT2(poems)\n",
        "    perplexity_loss = sum(perplexity_scores)\n",
        "    loss = torch.tensor(perplexity_loss, requires_grad=True)\n",
        "    return (loss, poems) if return_outputs else loss"
      ],
      "outputs": [],
      "metadata": {
        "id": "bw5jPN3VDHpM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "args = TrainingArguments(\n",
        "    f\"modelfolder\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    learning_rate=2e-8,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "JHQQWZmpA34H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "trainer = PerplexityTrainer(\n",
        "    # model_init=model_init,                        \n",
        "    model=model,\n",
        "    args=args,                  \n",
        "    train_dataset=train_dataset,         \n",
        "    eval_dataset=val_dataset             \n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "iAlCAHr1j6Nl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "'''trainer.hyperparameter_search(\n",
        "    direction=\"maximize\", \n",
        "    backend=\"ray\", \n",
        "    n_trials=10\n",
        ")'''"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'trainer.hyperparameter_search(\\n    direction=\"maximize\", \\n    backend=\"ray\", \\n    n_trials=10\\n)'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZVmi-grXkVzE",
        "outputId": "55b5d797-b7be-4252-bfda-9118d2146e84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluating Model"
      ],
      "metadata": {
        "id": "Ootd96ziAzP9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# Data loader and number of training steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m         \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;31m# Setting up training control variables:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m             )\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mtrain_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         return DataLoader(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_get_train_sampler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torch_generator_available\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             elif (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0;32m--> 103\u001b[0;31m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "yJDFu9Qectm-",
        "outputId": "016b7bdb-fd32-4570-c673-01a6fd6c3c81"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "trainer.evaluate()"
      ],
      "outputs": [],
      "metadata": {
        "id": "huqycSPiES-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Model"
      ],
      "metadata": {
        "id": "YTarAtZ6378d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.push_to_hub(f\"maple-{model_name.lower()}\", use_auth_token=HF_HUB_API_TOKEN)\n",
        "# tokenizer.push_to_hub(f\"maple-{model_name.lower()}\", use_auth_token=HF_HUB_API_TOKEN)"
      ],
      "outputs": [],
      "metadata": {
        "id": "cWohLdD3ow0J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.save_pretrained(f\"/content/drive/MyDrive/Maple/maple-{model_name.lower()}\")\n",
        "trainer.save_model(f\"/content/drive/MyDrive/Maple/maple-{model_name.lower()}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "8t7gzHkZDqSv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "wrXmSY-uMYIy"
      }
    }
  ]
}